# Projeto para estudo sobre as capacidade de LLMs open-source

Este projeto foi criado em Python para explorar e interagir com modelos de linguagem de grande porte (LLMs) open-source atrav√©s de um servidor Ollama. Ele fornece um cliente para se conectar ao host do Ollama, listar modelos, criar modelos personalizados e iniciar sess√µes de chat.

A aplica√ß√£o est√° sendo desenvolvida com uma interface gr√°fica usando PySide6.

## Core Features

-   **Ollama Client**: Um cliente robusto para interagir com a API do Ollama.
-   **Host Connection**: Conecta-se a uma inst√¢ncia do servidor Ollama em execu√ß√£o.
-   **Model Management**: Lista os modelos dispon√≠veis no host.
-   **Chat Interaction**: Envia prompts e recebe respostas de qualquer modelo selecionado.
-   **Logging**: Configura√ß√£o de logger centralizado para rastreamento de eventos e depura√ß√£o.

## Getting Started

### Prerequisites

-   Python 3.11+
-   `pip` e `pip-tools`
-   Uma inst√¢ncia do [Ollama](https://ollama.com/) em execu√ß√£o.

### Installation

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/Gil-Mendes-Carelli/projeto-v1.git
    cd projeto-v1
    ```

2.  **Crie e ative um ambiente virtual:**
    ```bash
    # Para Windows
    python -m venv venv
    .\venv\Scripts\activate

    # Para macOS/Linux
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Instale as depend√™ncias:**
    O projeto usa `pip-tools` para gerenciamento de depend√™ncias. Os pacotes necess√°rios est√£o listados em `requirements.txt`.
    ```bash
    pip install -r requirements.txt
    ```

## Usage Example

Voc√™ pode usar o `OllamaClient` para interagir com seu servidor Ollama. Aqui est√° um exemplo b√°sico de como us√°-lo em um script Python:

```python
from llm_host.ollama_client import OllamaClient, OllamaConnectionError

# Substitua pela URL do seu host Ollama
OLLAMA_HOST = "http://localhost:11434"

try:
    # 1. Conecte-se ao servidor Ollama
    client = OllamaClient().connect_to_host(OLLAMA_HOST)

    # 2. Liste os modelos dispon√≠veis
    models_list = client.list_models()
    print("Modelos dispon√≠veis:")
    print(models_list)

    # 3. Inicie um chat com um modelo (ex: llama3)
    # Certifique-se de ter o modelo baixado: `ollama pull llama3`
    model_name = "llama3"
    prompt = "Explique a computa√ß√£o qu√¢ntica em termos simples."

    print(f"\nEnviando prompt para o modelo '{model_name}'...")
    response = client.chat(model_name, prompt)

    print("\nResposta do modelo:")
    print(response)

except OllamaConnectionError as e:
    print(f"Erro de conex√£o: {e}")
except Exception as e:
    print(f"Ocorreu um erro inesperado: {e}")
```

## Project Structure

```
‚îú‚îÄ‚îÄ llm-host/
‚îÇ   ‚îî‚îÄ‚îÄ ollama_client.py  # Cliente principal para interagir com a API do Ollama
‚îú‚îÄ‚îÄ logger_config.py      # Configura√ß√£o centralizada do logger
‚îú‚îÄ‚îÄ requirements.in       # Defini√ß√£o das depend√™ncias do projeto
‚îú‚îÄ‚îÄ requirements.txt      # Lista de pacotes congelados para instala√ß√£o
‚îî‚îÄ‚îÄ ...
```

## üó∫Ô∏è Roadmap / To-Do

-   [ ] Configurar interface gr√°fica inicial com **PySide6**
-   [ ] Definir arquitetura do projeto (m√≥dulos, pacotes, pastas)
-   [ ] Criar tela principal da aplica√ß√£o
-   [ ] Adicionar testes unit√°rios
-   [ ] Preparar documenta√ß√£o mais detalhada
-   [ ] Criar sistema de releases no GitHub
